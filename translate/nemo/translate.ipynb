{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/matejk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[NeMo W 2024-06-12 10:11:07 nlp_overrides:802] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
      "    Megatron-based models require Apex to function correctly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-12 10:11:55 tokenizer_utils:179] Getting YouTokenToMeTokenizer with model: /tmp/tmpykh0snj_/abc9d4e95fee471e8ff84ca15d818568_en_tokenizer.64000.BPE.model with r2l: False.\n",
      "[NeMo I 2024-06-12 10:11:55 tokenizer_utils:179] Getting YouTokenToMeTokenizer with model: /tmp/tmpykh0snj_/d0f346b27aa14336bca69b91867db036_sl_tokenizer.64000.BPE.model with r2l: False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-06-12 10:11:55 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    src_file_name: /data/cjvt/v1.2.6/train.en\n",
      "    tgt_file_name: /data/cjvt/v1.2.6/train.sl\n",
      "    use_tarred_dataset: true\n",
      "    tar_files: null\n",
      "    metadata_file:\n",
      "    - /data/cjvt/v1.2.6/en-sl/metadata.tokens.1024.json\n",
      "    lines_per_dataset_fragment: 1000000\n",
      "    num_batches_per_tarfile: 100\n",
      "    shard_strategy: scatter\n",
      "    tokens_in_batch: 1024\n",
      "    clean: true\n",
      "    max_seq_length: 512\n",
      "    min_seq_length: 1\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    reverse_lang_direction: false\n",
      "    load_from_tarred_dataset: false\n",
      "    metadata_path: null\n",
      "    tar_shuffle_n: 100\n",
      "    n_preproc_jobs: -2\n",
      "    tar_file_prefix: parallel\n",
      "    concat_sampling_technique: temperature\n",
      "    concat_sampling_temperature: 5\n",
      "    concat_sampling_probabilities: null\n",
      "    \n",
      "[NeMo W 2024-06-12 10:11:55 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    src_file_name: /data/cjvt/v1.2.6/validation.en\n",
      "    tgt_file_name: /data/cjvt/v1.2.6/validation.sl\n",
      "    use_tarred_dataset: false\n",
      "    tar_files: null\n",
      "    metadata_file: null\n",
      "    lines_per_dataset_fragment: 1000000\n",
      "    num_batches_per_tarfile: 1000\n",
      "    shard_strategy: scatter\n",
      "    tokens_in_batch: 1024\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    min_seq_length: 1\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    reverse_lang_direction: false\n",
      "    load_from_tarred_dataset: false\n",
      "    metadata_path: null\n",
      "    tar_shuffle_n: 100\n",
      "    n_preproc_jobs: -2\n",
      "    tar_file_prefix: parallel\n",
      "    concat_sampling_technique: temperature\n",
      "    concat_sampling_temperature: 5\n",
      "    concat_sampling_probabilities: null\n",
      "    \n",
      "[NeMo W 2024-06-12 10:11:55 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    src_file_name: /data/cjvt/v1.2.6/test.en\n",
      "    tgt_file_name: /data/cjvt/v1.2.6/test.sl\n",
      "    use_tarred_dataset: false\n",
      "    tar_files: null\n",
      "    metadata_file: null\n",
      "    lines_per_dataset_fragment: 1000000\n",
      "    num_batches_per_tarfile: 1000\n",
      "    shard_strategy: scatter\n",
      "    tokens_in_batch: 1024\n",
      "    clean: false\n",
      "    max_seq_length: 512\n",
      "    min_seq_length: 1\n",
      "    cache_ids: false\n",
      "    cache_data_per_node: false\n",
      "    use_cache: false\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    drop_last: false\n",
      "    pin_memory: true\n",
      "    num_workers: 8\n",
      "    reverse_lang_direction: false\n",
      "    load_from_tarred_dataset: false\n",
      "    metadata_path: null\n",
      "    tar_shuffle_n: 100\n",
      "    n_preproc_jobs: -2\n",
      "    tar_file_prefix: parallel\n",
      "    concat_sampling_technique: temperature\n",
      "    concat_sampling_temperature: 5\n",
      "    concat_sampling_probabilities: null\n",
      "    \n",
      "[NeMo W 2024-06-12 10:11:55 nlp_overrides:802] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
      "    Megatron-based models require Apex to function correctly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-06-12 10:12:02 nlp_overrides:1110] Model MTEncDecModel was successfully restored from /mnt/c/Users/kranj/Desktop/mag-delo/models/aayn_base.nemo.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from re import findall\n",
    "from langdetect import detect\n",
    "import torch\n",
    "from nemo.collections.nlp.models import MTEncDecModel\n",
    "from nemo.utils import logging\n",
    "import contextlib\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available() and hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast'):\n",
    "    autocast = torch.cuda.amp.autocast\n",
    "else:\n",
    "    @contextlib.contextmanager\n",
    "    def autocast():\n",
    "        yield\n",
    "\n",
    "from nltk import download, sent_tokenize\n",
    "download('punkt')\n",
    "\n",
    "_TEXT_LEN_LIMIT = 5000\n",
    "_TEXT_SPLIT_THRESHOLD = 1024\n",
    "_SPLIT_LEN = 512\n",
    "model = MTEncDecModel.restore_from(\"models/aayn_base.nemo\", map_location=\"cuda\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(item):\n",
    "  time0 = time()\n",
    "  #logging.info(f\" Q: {item}\")\n",
    "\n",
    "  if isinstance(item, str):\n",
    "    text = [item]\n",
    "  else:\n",
    "    text = item\n",
    "  text_len = sum(len(_text) for _text in text)\n",
    "  if text_len > _TEXT_LEN_LIMIT:\n",
    "    logging.warning(f'{text}, text length exceded {text_len}c [max {_TEXT_LEN_LIMIT}c]')\n",
    "\n",
    "  text_batch = []\n",
    "  text_batch_split = []\n",
    "  for _text in text:\n",
    "    if len(_text) > _TEXT_SPLIT_THRESHOLD:\n",
    "      _split_start = len(text_batch)\n",
    "      _sent = sent_tokenize(_text)\n",
    "      i = 0\n",
    "      while i < len(_sent):\n",
    "        j = i+1\n",
    "        while j < len(_sent) and len(' '.join(_sent[i:j])) < _SPLIT_LEN: j+=1\n",
    "        if len(' '.join(_sent[i:j])) > _TEXT_SPLIT_THRESHOLD:\n",
    "          _split=findall(rf'(.{{1,{_SPLIT_LEN}}})(?:\\s|$)',' '.join(_sent[i:j]))\n",
    "          text_batch.extend(_split)\n",
    "        else:\n",
    "          text_batch.append(' '.join(_sent[i:j]))\n",
    "        i = j\n",
    "      _split_end = len(text_batch)\n",
    "      text_batch_split.append((_split_start,_split_end))\n",
    "    else:\n",
    "      text_batch.append(_text)\n",
    "\n",
    "  #logging.debug(f' B: {text_batch}, BS: {text_batch_split}')\n",
    "\n",
    "\n",
    "  translation_batch = model.translate(text_batch)\n",
    "  #logging.debug(f' BT: {translation_batch}')\n",
    "\n",
    "  translation = []\n",
    "  _start = 0\n",
    "  for _split_start,_split_end in text_batch_split:\n",
    "    if _split_start != _start:\n",
    "      translation.extend(translation_batch[_start:_split_start])\n",
    "    translation.append(' '.join(translation_batch[_split_start:_split_end]))\n",
    "    _start = _split_end\n",
    "  if _start < len(translation_batch):\n",
    "    translation.extend(translation_batch[_start:])\n",
    "\n",
    "\n",
    "  #logging.debug(f'text_length: {text_len}c, duration: {round(time()-time0,2)}s')\n",
    "\n",
    "  torch.cuda.empty_cache()\n",
    "  return ' '.join(translation) if isinstance(text, str) else translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 63/63 [00:04<00:00, 13.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 13.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120741, 120741, 120741)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process(anc, res, labels, filters):\n",
    "    data = list()\n",
    "    for a, r_, l_ in zip(anc, res, labels):\n",
    "        for r, l in zip(r_, l_):\n",
    "            data.append((a, r, l))\n",
    "    \n",
    "    for f in filters:\n",
    "        data = list(filter(f, data))\n",
    "\n",
    "\n",
    "    out = defaultdict(lambda: [list(), list()])\n",
    "    for d in data:\n",
    "        out[tuple(d[0])][0].append(d[1])\n",
    "        out[tuple(d[0])][1].append(d[2])\n",
    "\n",
    "    anc_, res_, lab_ = list(), list(), list()\n",
    "    for o in out.items():\n",
    "        anc_.append(list(o[0]))\n",
    "        res_.append(o[1][0])\n",
    "        lab_.append(o[1][1])\n",
    "\n",
    "    return anc_, res_, lab_\n",
    "\n",
    "\n",
    "def filter_length(x, min_length=20, max_length=200):\n",
    "    return len(x[1])>min_length and len(x[1])<max_length and np.all([len(i)>min_length and len(i)<max_length for i in x[0]])\n",
    "\n",
    "def filter_numbers(x, max_numbers=5):\n",
    "    return sum([x[1].count(str(n)) for n in range(10)])<max_numbers and np.all([sum([i.count(str(n)) for n in range(10)])<max_numbers for i in x[0]])\n",
    "\n",
    "def filter_chars(x, max_chars=1, chars=\"@#$%^_+={}|<>;\"):\n",
    "    return sum([x[1].count(n) for n in chars])<max_chars and np.all([sum([i.count(n) for n in chars])<max_chars for i in x[0]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def filter_dataset(docs, labels, batch_size=2056, filters = [filter_length, filter_numbers, filter_chars]):\n",
    "    n = len(labels)\n",
    "    anc, res = docs[\"ancestors\"], docs[\"responses\"]\n",
    "    anc_, res_, lab_ = list(), list(), list()\n",
    "    for i in tqdm(range(0, n, batch_size)):\n",
    "        a, r, l = process(anc[i:i+batch_size], res[i:i+batch_size], labels[i:i+batch_size], filters)\n",
    "        anc_ += a\n",
    "        res_ += r\n",
    "        lab_ += l\n",
    "    out = dict()\n",
    "    out[\"ancestors\"] = anc_\n",
    "    out[\"responses\"] = res_\n",
    "    return out, lab_ \n",
    "\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"data/english/train\", \"rb\") as file:\n",
    "    train = pickle.load(file)\n",
    "with open(\"data/english/test\", \"rb\") as file:\n",
    "    test = pickle.load(file)\n",
    "\n",
    "with open(\"data/english/train_labels\", \"rb\") as file:\n",
    "    train_labels = pickle.load(file)\n",
    "with open(\"data/english/test_labels\", \"rb\") as file:\n",
    "    test_labels = pickle.load(file)\n",
    "    \n",
    "train, train_labels = filter_dataset(train, train_labels)\n",
    "test, test_labels = filter_dataset(test, test_labels)\n",
    "\n",
    "anc, res = train[\"ancestors\"], train[\"responses\"]\n",
    "anc += test[\"ancestors\"]\n",
    "res += test[\"responses\"]\n",
    "labels = train_labels + test_labels\n",
    "anc = [\" > \".join(a) for a in anc]\n",
    "\n",
    "len(anc), len(res), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_train = list()\n",
    "for i, a, r in tqdm(zip(range(len(anc)), anc, res), total=len(anc)):\n",
    "    \n",
    "    translated_train.append(translate_text(\" ; \".join([a, \" ; \".join(r)])))\n",
    "    #print(translated_train[-1][0])\n",
    "    #print(detect(translated_train[-1][0]))\n",
    "\n",
    "# with open(\"data/slovene/train_filtered\", \"wb\") as file:\n",
    "#     pickle.dump(translated_train, file)\n",
    "\n",
    "# test_translated = translate_docs(test)\n",
    "# with open(\"data/test_filtered_translated\", \"wb\") as file:\n",
    "#     pickle.dump(test_translated, file)\n",
    "\n",
    "# train_translated = translate_docs(train)\n",
    "# with open(\"data/train_filtered_translated\", \"wb\") as file:\n",
    "#     pickle.dump(train_translated, file)\n",
    "#translate_text(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 28619/28619 [1:39:10<00:00,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/onion_data\", \"rb\") as file:\n",
    "    onion = pickle.load(file)\n",
    "\n",
    "onion_data, onion_labels = [i[\"input\"] for i in onion], [i[\"label\"] for i in onion]\n",
    "onion_translated = list()\n",
    "for i, o in tqdm(enumerate(onion_data), total=len(onion_data)):\n",
    "    onion_translated.append(translate_text(o))\n",
    "    if i%1000 == 0:\n",
    "        with open(\"data/slovene/onion_translated\", \"wb\") as file:\n",
    "            pickle.dump(onion_translated, file)\n",
    "\n",
    "with open(\"data/slovene/onion_translated\", \"wb\") as file:\n",
    "    pickle.dump(onion_translated, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "\n",
    "with open(\"data/slovene/sarc_filtered_translated\", \"rb\") as file:\n",
    "    sarc_translated = [i[0] for i in pickle.load(file)]\n",
    "with open(\"data/slovene/sarc_filtered_labels\", \"rb\") as file:\n",
    "    sarc_labels = pickle.load(file)\n",
    "\n",
    "anc2, res2 = list(), list()\n",
    "for s_ in sarc_translated:\n",
    "    s_ = s_.replace(\";;\", \";\")\n",
    "    s = s_.split(\";\")\n",
    "    anc2.append(s[0])\n",
    "    res2.append(s[1:])\n",
    "\n",
    "d = zip(anc, res, anc2, res2, sarc_labels)\n",
    "def f(x):\n",
    "    a1, r1, a2, r2, l = x\n",
    "    try:\n",
    "        t = detect_langs(a2)[0]\n",
    "        lang, prob = t.lang, t.prob\n",
    "    except:\n",
    "        return None\n",
    "    try:\n",
    "        t2 = detect_langs(\" \".join(r2))[0]\n",
    "        lang2, prob2 = t2.lang, t2.prob\n",
    "    except:\n",
    "        return None\n",
    "    if lang == lang2 == \"sl\" and min(prob, prob2) > 0.99 and len(r1) == len(r2) == len(l):\n",
    "        return x, min(prob, prob2)\n",
    "    return None\n",
    "\n",
    "from multiprocessing import Pool\n",
    "with Pool(16) as p:\n",
    "    d = p.map(f, d)\n",
    "\n",
    "dataset = dict()\n",
    "dataset[\"eng_anc\"] = list()\n",
    "dataset[\"eng_res\"] = list()\n",
    "dataset[\"slo_anc\"] = list()\n",
    "dataset[\"slo_res\"] = list()\n",
    "dataset[\"labels\"] = list()\n",
    "dataset[\"prob\"] = list()\n",
    "for d_ in d:\n",
    "    if not d_: continue\n",
    "    data_, p = d_\n",
    "    a1, r1, a2, r2, l = data_\n",
    "    dataset[\"eng_anc\"].append(a1.replace(\">\", \"\"))\n",
    "    dataset[\"eng_res\"].append(r1)\n",
    "    dataset[\"slo_anc\"].append(a2.replace(\">\", \"\"))\n",
    "    dataset[\"slo_res\"].append(r2)\n",
    "    dataset[\"labels\"].append([int(l_) for l_ in l])\n",
    "    dataset[\"prob\"].append(p)\n",
    "\n",
    "s = np.argsort(dataset[\"prob\"])\n",
    "for k in dataset.keys():\n",
    "    dataset[k] = list(np.array(dataset[k], dtype=object)[s])\n",
    "\n",
    "with open(\"data/parallel/SARC_PARALLEL\", \"wb\") as file:\n",
    "    pickle.dump(dataset, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/slovene/onion_translated\", \"rb\") as file:\n",
    "    onion_translated = pickle.load(file)\n",
    "with open(\"data/onion_data\", \"rb\") as file:\n",
    "    onion_original = pickle.load(file)\n",
    "\n",
    "d = zip(onion_original, onion_translated)\n",
    "\n",
    "def f(x):\n",
    "    o, t = x\n",
    "    t = t[0]\n",
    "    l = o[\"label\"]\n",
    "    e = o[\"input\"]\n",
    "    try:\n",
    "        t_ = detect_langs(t)[0]\n",
    "        lang, prob = t_.lang, t_.prob\n",
    "    except:\n",
    "        return None\n",
    "    if lang==\"sl\" and prob>0.99:\n",
    "        return e, t, l, prob\n",
    "    return None\n",
    "\n",
    "with Pool(16) as p:\n",
    "    d = p.map(f, d)\n",
    "\n",
    "dataset_onion = dict()\n",
    "dataset_onion[\"eng\"] = list()\n",
    "dataset_onion[\"slo\"] = list()\n",
    "dataset_onion[\"labels\"] = list()\n",
    "dataset_onion[\"prob\"] = list()\n",
    "for d_ in d:\n",
    "    if not d_: continue\n",
    "    e, t, l, prob = d_\n",
    "    dataset_onion[\"eng\"].append(e)\n",
    "    dataset_onion[\"slo\"].append(t)\n",
    "    dataset_onion[\"labels\"].append(l)\n",
    "    dataset_onion[\"prob\"].append(prob)\n",
    "\n",
    "s = np.argsort(dataset_onion[\"prob\"])\n",
    "for k in dataset_onion.keys():\n",
    "    dataset_onion[k] = list(np.array(dataset_onion[k])[s])\n",
    "\n",
    "\n",
    "with open(\"data/parallel/ONION_PARALLEL\", \"wb\") as file:\n",
    "    pickle.dump(dataset_onion, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
